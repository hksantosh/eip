{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIGHTH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hksantosh/eip/blob/master/week2/ninth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8_Uzkut1Jby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "becca1c3-f4fa-4815-b71e-206b4f00f2dd"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "93cfaec4-664a-443b-8a91-670a776fb4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f303749f198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = utils.to_categorical(y_train, 10)\n",
        "Y_test = utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "5b3b584f-fe32-4a5f-b6b6-6654e9c5aed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "ed780302-ddcc-420f-d759-090e07323b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(16, 3, activation='relu', input_shape=(28,28,1), use_bias=False)) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(32, 3, activation='relu', use_bias=False)) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu', use_bias=False)) #24\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#12\n",
        "\n",
        "model.add(Convolution2D(32, 3, activation='relu', use_bias=False)) #10\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#model.add(Convolution2D(16, 3, activation='relu', use_bias=False)) #20\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu', use_bias=False))  #10\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))  #5\n",
        "\n",
        "model.add(Convolution2D(32, 3, activation='relu', use_bias=False)) #3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#model.add(Convolution2D(32, 3, activation='relu', use_bias=False)) #3\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 3, activation='relu', use_bias=False)) #1\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_291 (Conv2D)          (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_215 (Bat (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_215 (Dropout)        (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_292 (Conv2D)          (None, 24, 24, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_216 (Bat (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_216 (Dropout)        (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_293 (Conv2D)          (None, 24, 24, 10)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_294 (Conv2D)          (None, 10, 10, 32)        2880      \n",
            "_________________________________________________________________\n",
            "batch_normalization_217 (Bat (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_217 (Dropout)        (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_295 (Conv2D)          (None, 10, 10, 10)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_75 (MaxPooling (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_296 (Conv2D)          (None, 3, 3, 32)          2880      \n",
            "_________________________________________________________________\n",
            "batch_normalization_218 (Bat (None, 3, 3, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_218 (Dropout)        (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_297 (Conv2D)          (None, 1, 1, 10)          2880      \n",
            "_________________________________________________________________\n",
            "batch_normalization_219 (Bat (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_219 (Dropout)        (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,520\n",
            "Trainable params: 14,276\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "ed666bcc-53b5-432c-e528-79b6f69dfc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "callbacks = [\n",
        "  ModelCheckpoint(filepath='wk2_ninth.h5', save_best_only=True, monitor='val_accuracy', verbose=1),\n",
        "  LearningRateScheduler(scheduler, verbose=1)\n",
        "]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=callbacks)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0015.\n",
            "Epoch 1/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.8587\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98000, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5335 - accuracy: 0.8590 - val_loss: 0.1044 - val_accuracy: 0.9800\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0011372252.\n",
            "Epoch 2/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.9115\n",
            "Epoch 00002: val_accuracy improved from 0.98000 to 0.98330, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.3232 - accuracy: 0.9115 - val_loss: 0.0745 - val_accuracy: 0.9833\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0009157509.\n",
            "Epoch 3/20\n",
            "59744/60000 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.9219\n",
            "Epoch 00003: val_accuracy did not improve from 0.98330\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.2680 - accuracy: 0.9220 - val_loss: 0.1128 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007664793.\n",
            "Epoch 4/20\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9259\n",
            "Epoch 00004: val_accuracy improved from 0.98330 to 0.98830, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 166us/sample - loss: 0.2450 - accuracy: 0.9260 - val_loss: 0.0475 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000659051.\n",
            "Epoch 5/20\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9322\n",
            "Epoch 00005: val_accuracy improved from 0.98830 to 0.98880, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.2178 - accuracy: 0.9322 - val_loss: 0.0448 - val_accuracy: 0.9888\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005780347.\n",
            "Epoch 6/20\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9315\n",
            "Epoch 00006: val_accuracy improved from 0.98880 to 0.98960, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.2149 - accuracy: 0.9316 - val_loss: 0.0408 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005147563.\n",
            "Epoch 7/20\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9338\n",
            "Epoch 00007: val_accuracy improved from 0.98960 to 0.99030, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.2077 - accuracy: 0.9339 - val_loss: 0.0360 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004639654.\n",
            "Epoch 8/20\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9340\n",
            "Epoch 00008: val_accuracy improved from 0.99030 to 0.99110, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 162us/sample - loss: 0.1975 - accuracy: 0.9340 - val_loss: 0.0335 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004222973.\n",
            "Epoch 9/20\n",
            "59744/60000 [============================>.] - ETA: 0s - loss: 0.1921 - accuracy: 0.9362\n",
            "Epoch 00009: val_accuracy improved from 0.99110 to 0.99170, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.1920 - accuracy: 0.9362 - val_loss: 0.0347 - val_accuracy: 0.9917\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0003874968.\n",
            "Epoch 10/20\n",
            "59840/60000 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9380\n",
            "Epoch 00010: val_accuracy did not improve from 0.99170\n",
            "60000/60000 [==============================] - 10s 166us/sample - loss: 0.1849 - accuracy: 0.9380 - val_loss: 0.0322 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0003579952.\n",
            "Epoch 11/20\n",
            "59808/60000 [============================>.] - ETA: 0s - loss: 0.1852 - accuracy: 0.9381\n",
            "Epoch 00011: val_accuracy did not improve from 0.99170\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.1851 - accuracy: 0.9381 - val_loss: 0.0321 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000332668.\n",
            "Epoch 12/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9377\n",
            "Epoch 00012: val_accuracy improved from 0.99170 to 0.99200, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 161us/sample - loss: 0.1781 - accuracy: 0.9377 - val_loss: 0.0316 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0003106877.\n",
            "Epoch 13/20\n",
            "59648/60000 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9388\n",
            "Epoch 00013: val_accuracy improved from 0.99200 to 0.99220, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 162us/sample - loss: 0.1768 - accuracy: 0.9387 - val_loss: 0.0302 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002914319.\n",
            "Epoch 14/20\n",
            "59680/60000 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9404\n",
            "Epoch 00014: val_accuracy improved from 0.99220 to 0.99270, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.1739 - accuracy: 0.9406 - val_loss: 0.0308 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0002744237.\n",
            "Epoch 15/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9407\n",
            "Epoch 00015: val_accuracy did not improve from 0.99270\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.1717 - accuracy: 0.9408 - val_loss: 0.0291 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002592913.\n",
            "Epoch 16/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9388\n",
            "Epoch 00016: val_accuracy did not improve from 0.99270\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.1767 - accuracy: 0.9388 - val_loss: 0.0311 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0002457405.\n",
            "Epoch 17/20\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9411\n",
            "Epoch 00017: val_accuracy did not improve from 0.99270\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 0.1717 - accuracy: 0.9410 - val_loss: 0.0312 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0002335357.\n",
            "Epoch 18/20\n",
            "59968/60000 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9412\n",
            "Epoch 00018: val_accuracy improved from 0.99270 to 0.99340, saving model to wk2_ninth.h5\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 0.1689 - accuracy: 0.9412 - val_loss: 0.0275 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002224859.\n",
            "Epoch 19/20\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9408\n",
            "Epoch 00019: val_accuracy did not improve from 0.99340\n",
            "60000/60000 [==============================] - 9s 157us/sample - loss: 0.1743 - accuracy: 0.9409 - val_loss: 0.0287 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0002124345.\n",
            "Epoch 20/20\n",
            "59776/60000 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9413\n",
            "Epoch 00020: val_accuracy did not improve from 0.99340\n",
            "60000/60000 [==============================] - 10s 159us/sample - loss: 0.1661 - accuracy: 0.9413 - val_loss: 0.0260 - val_accuracy: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f302bc3b860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxlW9ufyQiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Score for the best model\n",
        "score = models.load_model('wk2_ninth.h5').evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRvxdRsM7-FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}