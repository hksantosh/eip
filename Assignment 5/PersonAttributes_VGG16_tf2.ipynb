{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttributes_VGG16_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hksantosh/eip/blob/master/Assignment%205/PersonAttributes_VGG16_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjpmKhrzJmrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=[]\n",
        "while(1):\n",
        "  d.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "9df579ad-f087-4729-876a-6acce3a59521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/EIP4/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "0883c1ce-1bec-48f1-f6e0-d1f84db7bb50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import os, cv2, json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Input, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, SeparableConv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "2e1ef5b9-62cf-4612-f1d4-d03e11ff369a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "015ff0ef-66b9-4c1b-b768-33e3d31ad204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])        \n",
        "        if self.augmentation is not None:\n",
        "            images = self.augmentation.flow(images, shuffle=False).next()\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        \n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "423e76a5-36a8-4d98-9b8f-b7fa75ae56df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "12af99af-792c-4c22-92dd-317bfb795f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4637</th>\n",
              "      <td>resized/4638.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12208</th>\n",
              "      <td>resized/12210.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10222</th>\n",
              "      <td>resized/10224.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12621</th>\n",
              "      <td>resized/12623.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>resized/1067.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "4637    resized/4638.jpg              0  ...                        0              1\n",
              "12208  resized/12210.jpg              0  ...                        1              0\n",
              "10222  resized/10224.jpg              0  ...                        0              0\n",
              "12621  resized/12623.jpg              0  ...                        0              0\n",
              "1066    resized/1067.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybCpiNPSe7Gn",
        "colab_type": "code",
        "outputId": "46d515b7-f982-4fba-ad33-b7f2e2fa7a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13270</th>\n",
              "      <td>resized/13272.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6573</th>\n",
              "      <td>resized/6574.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>resized/2837.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11323</th>\n",
              "      <td>resized/11325.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>resized/6537.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "13270  resized/13272.jpg              0  ...                        1              0\n",
              "6573    resized/6574.jpg              0  ...                        1              0\n",
              "2836    resized/2837.jpg              1  ...                        1              0\n",
              "11323  resized/11325.jpg              1  ...                        1              0\n",
              "6536    resized/6537.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "outputId": "f5b4ece8-4e80-4e86-9857-70936acfa7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create train and validation data generators\n",
        "#train_gen = PersonDataGenerator(train_df, batch_size=64)\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=32, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        #rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function =\n",
        "                    get_random_eraser(v_l=0, v_h=255, s_l=0.02, s_h=0.05, \n",
        "                                                 r_1=0.75, r_2=1/0.75, pixel_level=True)\n",
        "    )\n",
        ")\n",
        "\n",
        "#valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)\n",
        "valid_gen = PersonDataGenerator(\n",
        "    val_df, \n",
        "    batch_size=32, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        #rescale=1./255\n",
        "    )\n",
        ")\n",
        "\n",
        "len(train_gen), len(valid_gen)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "5f9a9336-49a6-422b-a9e8-ab676f2410a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wxE3O3Tfg9m",
        "colab_type": "code",
        "outputId": "c098a06c-f294-44f7-c577-26d8c25e26eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "images.shape, targets.get('gender_output').shape, targets.get('age_output').shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 224, 224, 3), (32, 2), (32, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(128, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dense(128, activation=\"relu\")(in_layer)\n",
        "    neck = BatchNormalization(momentum=0.3)(neck)\n",
        "    neck = Dropout(0.06)(neck)\n",
        "    #neck = BatchNormalization(momentum=0.2)(neck)\n",
        "    #neck = Dropout(0.03)(neck)\n",
        "    #neck = Dense(256, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4FcGop4gTXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSswlrOPR7Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 160:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "# monitor the learning rate\n",
        "class LearningRateMonitor(Callback):\n",
        "  # start of training\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.lrates = list()\n",
        "\n",
        "  # end of each training epoch\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    # get and store the learning rate\n",
        "    optimizer = self.model.optimizer\n",
        "    lrate = float(backend.get_value(self.model.optimizer.lr))\n",
        "    print('Learning rate used for the completed epoch:', lrate)\n",
        "    self.lrates.append(lrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zNWK1rwBLGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath='/content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5',\n",
        "                             monitor='val_loss',\n",
        "                             verbose=2,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "lrm = LearningRateMonitor()\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0,\n",
        "                               patience=3, min_lr=0.1e-7)\n",
        "\n",
        "#callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint, lr_reducer, lrm]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        " \t\"bag_output\": \"categorical_crossentropy\",\n",
        " \t\"footwear_output\": \"categorical_crossentropy\",\n",
        " \t\"pose_output\": \"categorical_crossentropy\",\n",
        " \t\"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\n",
        "    \"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\": 1.0,\n",
        "    \"bag_output\": 1.0,  \"footwear_output\": 1.0,  \"pose_output\": 1.0,  \"emotion_output\": 1.0\n",
        "    }\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "#opt = SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=losses,\n",
        "    loss_weights=loss_weights,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "47d68705-dc03-493f-efcb-094fc2d00967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "#    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=200,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-122-bc74cf440d61>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  {'gender_output': '...', 'image_quality_output': '...', 'age_output': '...', 'weight_output': '...', 'bag_output': '...', 'pose_output': '...', 'footwear_output': '...', 'emotion_output': '...'}\n",
            "    to  \n",
            "  ['...', '...', '...', '...', '...', '...', '...', '...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  {'gender_output': '...', 'image_quality_output': '...', 'age_output': '...', 'weight_output': '...', 'bag_output': '...', 'pose_output': '...', 'footwear_output': '...', 'emotion_output': '...'}\n",
            "    to  \n",
            "  ['...', '...', '...', '...', '...', '...', '...', '...']\n",
            "Train for 360 steps, validate for 63 steps\n",
            "Epoch 1/200\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 12.33429, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 101s - loss: 8.2849 - gender_output_loss: 0.6873 - image_quality_output_loss: 1.0235 - age_output_loss: 1.5028 - weight_output_loss: 1.0869 - bag_output_loss: 0.9725 - footwear_output_loss: 1.0051 - pose_output_loss: 0.9884 - emotion_output_loss: 1.0184 - gender_output_accuracy: 0.5571 - image_quality_output_accuracy: 0.5161 - age_output_accuracy: 0.3582 - weight_output_accuracy: 0.5918 - bag_output_accuracy: 0.5213 - footwear_output_accuracy: 0.5107 - pose_output_accuracy: 0.5726 - emotion_output_accuracy: 0.6651 - val_loss: 12.3343 - val_gender_output_loss: 0.6736 - val_image_quality_output_loss: 0.9703 - val_age_output_loss: 1.6490 - val_weight_output_loss: 0.9725 - val_bag_output_loss: 0.9104 - val_footwear_output_loss: 0.9691 - val_pose_output_loss: 5.2594 - val_emotion_output_loss: 0.9300 - val_gender_output_accuracy: 0.5580 - val_image_quality_output_accuracy: 0.5541 - val_age_output_accuracy: 0.3849 - val_weight_output_accuracy: 0.6389 - val_bag_output_accuracy: 0.5595 - val_footwear_output_accuracy: 0.5496 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 00002: val_loss improved from 12.33429 to 9.78833, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.8570 - gender_output_loss: 0.6768 - image_quality_output_loss: 0.9947 - age_output_loss: 1.4386 - weight_output_loss: 0.9961 - bag_output_loss: 0.9262 - footwear_output_loss: 0.9650 - pose_output_loss: 0.9427 - emotion_output_loss: 0.9170 - gender_output_accuracy: 0.5699 - image_quality_output_accuracy: 0.5417 - age_output_accuracy: 0.3853 - weight_output_accuracy: 0.6288 - bag_output_accuracy: 0.5497 - footwear_output_accuracy: 0.5424 - pose_output_accuracy: 0.6101 - emotion_output_accuracy: 0.7119 - val_loss: 9.7883 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 2.7660 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 1.5121 - val_footwear_output_loss: 0.9444 - val_pose_output_loss: 0.9919 - val_emotion_output_loss: 0.9250 - val_gender_output_accuracy: 0.5719 - val_image_quality_output_accuracy: 0.5551 - val_age_output_accuracy: 0.3904 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5655 - val_footwear_output_accuracy: 0.5630 - val_pose_output_accuracy: 0.6210 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 00003: val_loss improved from 9.78833 to 9.43157, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.8349 - gender_output_loss: 0.6697 - image_quality_output_loss: 0.9945 - age_output_loss: 1.4313 - weight_output_loss: 0.9948 - bag_output_loss: 0.9267 - footwear_output_loss: 0.9631 - pose_output_loss: 0.9387 - emotion_output_loss: 0.9162 - gender_output_accuracy: 0.5830 - image_quality_output_accuracy: 0.5425 - age_output_accuracy: 0.3899 - weight_output_accuracy: 0.6315 - bag_output_accuracy: 0.5497 - footwear_output_accuracy: 0.5475 - pose_output_accuracy: 0.6125 - emotion_output_accuracy: 0.7117 - val_loss: 9.4316 - val_gender_output_loss: 0.6716 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4222 - val_weight_output_loss: 2.0253 - val_bag_output_loss: 1.2952 - val_footwear_output_loss: 1.2024 - val_pose_output_loss: 0.9202 - val_emotion_output_loss: 0.9209 - val_gender_output_accuracy: 0.5749 - val_image_quality_output_accuracy: 0.5531 - val_age_output_accuracy: 0.3904 - val_weight_output_accuracy: 0.6369 - val_bag_output_accuracy: 0.5541 - val_footwear_output_accuracy: 0.5322 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 9.43157\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 7.8097 - gender_output_loss: 0.6665 - image_quality_output_loss: 0.9929 - age_output_loss: 1.4284 - weight_output_loss: 0.9928 - bag_output_loss: 0.9226 - footwear_output_loss: 0.9533 - pose_output_loss: 0.9383 - emotion_output_loss: 0.9149 - gender_output_accuracy: 0.5833 - image_quality_output_accuracy: 0.5430 - age_output_accuracy: 0.3891 - weight_output_accuracy: 0.6306 - bag_output_accuracy: 0.5518 - footwear_output_accuracy: 0.5475 - pose_output_accuracy: 0.6127 - emotion_output_accuracy: 0.7112 - val_loss: 440.4094 - val_gender_output_loss: 1.0758 - val_image_quality_output_loss: 18.7668 - val_age_output_loss: 1.5511 - val_weight_output_loss: 77.8122 - val_bag_output_loss: 2.6742 - val_footwear_output_loss: 5.4532 - val_pose_output_loss: 264.1124 - val_emotion_output_loss: 68.9637 - val_gender_output_accuracy: 0.5734 - val_image_quality_output_accuracy: 0.5417 - val_age_output_accuracy: 0.3824 - val_weight_output_accuracy: 0.6260 - val_bag_output_accuracy: 0.4926 - val_footwear_output_accuracy: 0.5526 - val_pose_output_accuracy: 0.5556 - val_emotion_output_accuracy: 0.6880\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 00005: val_loss improved from 9.43157 to 7.76341, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.8365 - gender_output_loss: 0.6781 - image_quality_output_loss: 0.9935 - age_output_loss: 1.4262 - weight_output_loss: 0.9928 - bag_output_loss: 0.9260 - footwear_output_loss: 0.9665 - pose_output_loss: 0.9386 - emotion_output_loss: 0.9147 - gender_output_accuracy: 0.5675 - image_quality_output_accuracy: 0.5433 - age_output_accuracy: 0.3926 - weight_output_accuracy: 0.6302 - bag_output_accuracy: 0.5490 - footwear_output_accuracy: 0.5431 - pose_output_accuracy: 0.6134 - emotion_output_accuracy: 0.7122 - val_loss: 7.7634 - val_gender_output_loss: 0.6717 - val_image_quality_output_loss: 0.9735 - val_age_output_loss: 1.4318 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.9187 - val_footwear_output_loss: 0.9574 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.9220 - val_gender_output_accuracy: 0.5630 - val_image_quality_output_accuracy: 0.5551 - val_age_output_accuracy: 0.3874 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5615 - val_footwear_output_accuracy: 0.5511 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.76341\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 7.8069 - gender_output_loss: 0.6711 - image_quality_output_loss: 0.9924 - age_output_loss: 1.4232 - weight_output_loss: 0.9920 - bag_output_loss: 0.9230 - footwear_output_loss: 0.9581 - pose_output_loss: 0.9358 - emotion_output_loss: 0.9113 - gender_output_accuracy: 0.5740 - image_quality_output_accuracy: 0.5448 - age_output_accuracy: 0.3909 - weight_output_accuracy: 0.6318 - bag_output_accuracy: 0.5494 - footwear_output_accuracy: 0.5493 - pose_output_accuracy: 0.6130 - emotion_output_accuracy: 0.7124 - val_loss: 8.8764 - val_gender_output_loss: 0.6670 - val_image_quality_output_loss: 2.0634 - val_age_output_loss: 1.4250 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.9069 - val_footwear_output_loss: 0.9871 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9214 - val_gender_output_accuracy: 0.5749 - val_image_quality_output_accuracy: 0.5526 - val_age_output_accuracy: 0.3914 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5675 - val_footwear_output_accuracy: 0.5278 - val_pose_output_accuracy: 0.6200 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.76341 to 7.74871, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.7783 - gender_output_loss: 0.6651 - image_quality_output_loss: 0.9919 - age_output_loss: 1.4205 - weight_output_loss: 0.9918 - bag_output_loss: 0.9182 - footwear_output_loss: 0.9462 - pose_output_loss: 0.9345 - emotion_output_loss: 0.9102 - gender_output_accuracy: 0.5821 - image_quality_output_accuracy: 0.5447 - age_output_accuracy: 0.3926 - weight_output_accuracy: 0.6314 - bag_output_accuracy: 0.5535 - footwear_output_accuracy: 0.5647 - pose_output_accuracy: 0.6151 - emotion_output_accuracy: 0.7122 - val_loss: 7.7487 - val_gender_output_loss: 0.6643 - val_image_quality_output_loss: 0.9688 - val_age_output_loss: 1.4182 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9506 - val_footwear_output_loss: 0.9231 - val_pose_output_loss: 0.9207 - val_emotion_output_loss: 0.9259 - val_gender_output_accuracy: 0.5848 - val_image_quality_output_accuracy: 0.5521 - val_age_output_accuracy: 0.3859 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5665 - val_footwear_output_accuracy: 0.5774 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.74871 to 7.72857, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.7413 - gender_output_loss: 0.6601 - image_quality_output_loss: 0.9908 - age_output_loss: 1.4162 - weight_output_loss: 0.9900 - bag_output_loss: 0.9157 - footwear_output_loss: 0.9269 - pose_output_loss: 0.9322 - emotion_output_loss: 0.9094 - gender_output_accuracy: 0.5969 - image_quality_output_accuracy: 0.5456 - age_output_accuracy: 0.3909 - weight_output_accuracy: 0.6305 - bag_output_accuracy: 0.5595 - footwear_output_accuracy: 0.5739 - pose_output_accuracy: 0.6158 - emotion_output_accuracy: 0.7124 - val_loss: 7.7286 - val_gender_output_loss: 0.6566 - val_image_quality_output_loss: 0.9735 - val_age_output_loss: 1.4166 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9521 - val_footwear_output_loss: 0.9110 - val_pose_output_loss: 0.9172 - val_emotion_output_loss: 0.9244 - val_gender_output_accuracy: 0.5962 - val_image_quality_output_accuracy: 0.5506 - val_age_output_accuracy: 0.3864 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5516 - val_footwear_output_accuracy: 0.5838 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7024\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.72857 to 7.65786, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.7099 - gender_output_loss: 0.6551 - image_quality_output_loss: 0.9893 - age_output_loss: 1.4139 - weight_output_loss: 0.9889 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9111 - pose_output_loss: 0.9293 - emotion_output_loss: 0.9094 - gender_output_accuracy: 0.6036 - image_quality_output_accuracy: 0.5484 - age_output_accuracy: 0.3938 - weight_output_accuracy: 0.6321 - bag_output_accuracy: 0.5534 - footwear_output_accuracy: 0.5817 - pose_output_accuracy: 0.6156 - emotion_output_accuracy: 0.7126 - val_loss: 7.6579 - val_gender_output_loss: 0.6472 - val_image_quality_output_loss: 0.9792 - val_age_output_loss: 1.4286 - val_weight_output_loss: 0.9676 - val_bag_output_loss: 0.9124 - val_footwear_output_loss: 0.8919 - val_pose_output_loss: 0.9117 - val_emotion_output_loss: 0.9191 - val_gender_output_accuracy: 0.6190 - val_image_quality_output_accuracy: 0.5551 - val_age_output_accuracy: 0.3839 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5724 - val_footwear_output_accuracy: 0.5967 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.65786\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 7.7035 - gender_output_loss: 0.6452 - image_quality_output_loss: 0.9887 - age_output_loss: 1.4144 - weight_output_loss: 0.9925 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9114 - pose_output_loss: 0.9302 - emotion_output_loss: 0.9083 - gender_output_accuracy: 0.6161 - image_quality_output_accuracy: 0.5475 - age_output_accuracy: 0.3931 - weight_output_accuracy: 0.6327 - bag_output_accuracy: 0.5565 - footwear_output_accuracy: 0.5804 - pose_output_accuracy: 0.6146 - emotion_output_accuracy: 0.7128 - val_loss: 7.6900 - val_gender_output_loss: 0.6555 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4197 - val_weight_output_loss: 0.9691 - val_bag_output_loss: 0.9067 - val_footwear_output_loss: 0.9276 - val_pose_output_loss: 0.9127 - val_emotion_output_loss: 0.9250 - val_gender_output_accuracy: 0.6027 - val_image_quality_output_accuracy: 0.5551 - val_age_output_accuracy: 0.3919 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5689 - val_footwear_output_accuracy: 0.5739 - val_pose_output_accuracy: 0.6215 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: val_loss improved from 7.65786 to 7.60669, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.6608 - gender_output_loss: 0.6402 - image_quality_output_loss: 0.9866 - age_output_loss: 1.4130 - weight_output_loss: 0.9852 - bag_output_loss: 0.9073 - footwear_output_loss: 0.8989 - pose_output_loss: 0.9217 - emotion_output_loss: 0.9079 - gender_output_accuracy: 0.6240 - image_quality_output_accuracy: 0.5470 - age_output_accuracy: 0.3951 - weight_output_accuracy: 0.6330 - bag_output_accuracy: 0.5623 - footwear_output_accuracy: 0.5893 - pose_output_accuracy: 0.6145 - emotion_output_accuracy: 0.7128 - val_loss: 7.6067 - val_gender_output_loss: 0.6324 - val_image_quality_output_loss: 0.9679 - val_age_output_loss: 1.4353 - val_weight_output_loss: 0.9731 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 0.8766 - val_pose_output_loss: 0.8929 - val_emotion_output_loss: 0.9176 - val_gender_output_accuracy: 0.6409 - val_image_quality_output_accuracy: 0.5541 - val_age_output_accuracy: 0.3914 - val_weight_output_accuracy: 0.6389 - val_bag_output_accuracy: 0.5590 - val_footwear_output_accuracy: 0.6111 - val_pose_output_accuracy: 0.6210 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.60669 to 7.43764, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.5240 - gender_output_loss: 0.6080 - image_quality_output_loss: 0.9735 - age_output_loss: 1.4086 - weight_output_loss: 0.9783 - bag_output_loss: 0.8956 - footwear_output_loss: 0.8572 - pose_output_loss: 0.8973 - emotion_output_loss: 0.9055 - gender_output_accuracy: 0.6575 - image_quality_output_accuracy: 0.5499 - age_output_accuracy: 0.3938 - weight_output_accuracy: 0.6326 - bag_output_accuracy: 0.5726 - footwear_output_accuracy: 0.6149 - pose_output_accuracy: 0.6148 - emotion_output_accuracy: 0.7131 - val_loss: 7.4376 - val_gender_output_loss: 0.5818 - val_image_quality_output_loss: 0.9566 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9646 - val_bag_output_loss: 0.8862 - val_footwear_output_loss: 0.8425 - val_pose_output_loss: 0.8717 - val_emotion_output_loss: 0.9204 - val_gender_output_accuracy: 0.7004 - val_image_quality_output_accuracy: 0.5531 - val_age_output_accuracy: 0.3894 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.5833 - val_footwear_output_accuracy: 0.6270 - val_pose_output_accuracy: 0.6205 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.43764 to 7.19185, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 7.3478 - gender_output_loss: 0.5725 - image_quality_output_loss: 0.9608 - age_output_loss: 1.4006 - weight_output_loss: 0.9761 - bag_output_loss: 0.8841 - footwear_output_loss: 0.8396 - pose_output_loss: 0.8150 - emotion_output_loss: 0.8991 - gender_output_accuracy: 0.6938 - image_quality_output_accuracy: 0.5498 - age_output_accuracy: 0.3956 - weight_output_accuracy: 0.6345 - bag_output_accuracy: 0.5835 - footwear_output_accuracy: 0.6277 - pose_output_accuracy: 0.6438 - emotion_output_accuracy: 0.7125 - val_loss: 7.1919 - val_gender_output_loss: 0.5461 - val_image_quality_output_loss: 0.9488 - val_age_output_loss: 1.4245 - val_weight_output_loss: 0.9533 - val_bag_output_loss: 0.8785 - val_footwear_output_loss: 0.8096 - val_pose_output_loss: 0.7164 - val_emotion_output_loss: 0.9148 - val_gender_output_accuracy: 0.7361 - val_image_quality_output_accuracy: 0.5595 - val_age_output_accuracy: 0.3819 - val_weight_output_accuracy: 0.6394 - val_bag_output_accuracy: 0.6037 - val_footwear_output_accuracy: 0.6349 - val_pose_output_accuracy: 0.7063 - val_emotion_output_accuracy: 0.7004\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.19185\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 7.1436 - gender_output_loss: 0.5376 - image_quality_output_loss: 0.9511 - age_output_loss: 1.3916 - weight_output_loss: 0.9695 - bag_output_loss: 0.8767 - footwear_output_loss: 0.8245 - pose_output_loss: 0.7049 - emotion_output_loss: 0.8878 - gender_output_accuracy: 0.7214 - image_quality_output_accuracy: 0.5490 - age_output_accuracy: 0.3942 - weight_output_accuracy: 0.6321 - bag_output_accuracy: 0.5918 - footwear_output_accuracy: 0.6358 - pose_output_accuracy: 0.7008 - emotion_output_accuracy: 0.7126 - val_loss: 7.1980 - val_gender_output_loss: 0.5107 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.4109 - val_weight_output_loss: 0.9725 - val_bag_output_loss: 0.8844 - val_footwear_output_loss: 0.8300 - val_pose_output_loss: 0.6888 - val_emotion_output_loss: 0.9273 - val_gender_output_accuracy: 0.7440 - val_image_quality_output_accuracy: 0.5382 - val_age_output_accuracy: 0.3889 - val_weight_output_accuracy: 0.6369 - val_bag_output_accuracy: 0.5903 - val_footwear_output_accuracy: 0.6215 - val_pose_output_accuracy: 0.7257 - val_emotion_output_accuracy: 0.6890\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.19185 to 6.90449, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.9784 - gender_output_loss: 0.5011 - image_quality_output_loss: 0.9421 - age_output_loss: 1.3784 - weight_output_loss: 0.9583 - bag_output_loss: 0.8613 - footwear_output_loss: 0.8098 - pose_output_loss: 0.6497 - emotion_output_loss: 0.8777 - gender_output_accuracy: 0.7523 - image_quality_output_accuracy: 0.5504 - age_output_accuracy: 0.3997 - weight_output_accuracy: 0.6354 - bag_output_accuracy: 0.6040 - footwear_output_accuracy: 0.6446 - pose_output_accuracy: 0.7273 - emotion_output_accuracy: 0.7127 - val_loss: 6.9045 - val_gender_output_loss: 0.4825 - val_image_quality_output_loss: 0.9434 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9455 - val_bag_output_loss: 0.8504 - val_footwear_output_loss: 0.7798 - val_pose_output_loss: 0.6094 - val_emotion_output_loss: 0.8980 - val_gender_output_accuracy: 0.7698 - val_image_quality_output_accuracy: 0.5645 - val_age_output_accuracy: 0.3864 - val_weight_output_accuracy: 0.6414 - val_bag_output_accuracy: 0.6101 - val_footwear_output_accuracy: 0.6538 - val_pose_output_accuracy: 0.7564 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 00016: val_loss improved from 6.90449 to 6.86858, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.8627 - gender_output_loss: 0.4679 - image_quality_output_loss: 0.9347 - age_output_loss: 1.3715 - weight_output_loss: 0.9547 - bag_output_loss: 0.8499 - footwear_output_loss: 0.7918 - pose_output_loss: 0.6171 - emotion_output_loss: 0.8751 - gender_output_accuracy: 0.7739 - image_quality_output_accuracy: 0.5563 - age_output_accuracy: 0.4010 - weight_output_accuracy: 0.6357 - bag_output_accuracy: 0.6109 - footwear_output_accuracy: 0.6511 - pose_output_accuracy: 0.7429 - emotion_output_accuracy: 0.7126 - val_loss: 6.8686 - val_gender_output_loss: 0.4966 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.3777 - val_weight_output_loss: 0.9392 - val_bag_output_loss: 0.8535 - val_footwear_output_loss: 0.7722 - val_pose_output_loss: 0.5943 - val_emotion_output_loss: 0.8983 - val_gender_output_accuracy: 0.7594 - val_image_quality_output_accuracy: 0.5655 - val_age_output_accuracy: 0.4043 - val_weight_output_accuracy: 0.6314 - val_bag_output_accuracy: 0.6101 - val_footwear_output_accuracy: 0.6572 - val_pose_output_accuracy: 0.7619 - val_emotion_output_accuracy: 0.7029\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: val_loss improved from 6.86858 to 6.81835, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.7831 - gender_output_loss: 0.4548 - image_quality_output_loss: 0.9258 - age_output_loss: 1.3655 - weight_output_loss: 0.9462 - bag_output_loss: 0.8437 - footwear_output_loss: 0.7844 - pose_output_loss: 0.5897 - emotion_output_loss: 0.8729 - gender_output_accuracy: 0.7883 - image_quality_output_accuracy: 0.5567 - age_output_accuracy: 0.4024 - weight_output_accuracy: 0.6364 - bag_output_accuracy: 0.6190 - footwear_output_accuracy: 0.6598 - pose_output_accuracy: 0.7572 - emotion_output_accuracy: 0.7128 - val_loss: 6.8183 - val_gender_output_loss: 0.4501 - val_image_quality_output_loss: 0.9422 - val_age_output_loss: 1.3872 - val_weight_output_loss: 0.9385 - val_bag_output_loss: 0.8499 - val_footwear_output_loss: 0.7815 - val_pose_output_loss: 0.5734 - val_emotion_output_loss: 0.8956 - val_gender_output_accuracy: 0.7763 - val_image_quality_output_accuracy: 0.5546 - val_age_output_accuracy: 0.3993 - val_weight_output_accuracy: 0.6419 - val_bag_output_accuracy: 0.6205 - val_footwear_output_accuracy: 0.6483 - val_pose_output_accuracy: 0.7748 - val_emotion_output_accuracy: 0.7014\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 00018: val_loss improved from 6.81835 to 6.66088, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.6676 - gender_output_loss: 0.4184 - image_quality_output_loss: 0.9143 - age_output_loss: 1.3609 - weight_output_loss: 0.9392 - bag_output_loss: 0.8326 - footwear_output_loss: 0.7759 - pose_output_loss: 0.5580 - emotion_output_loss: 0.8682 - gender_output_accuracy: 0.8073 - image_quality_output_accuracy: 0.5566 - age_output_accuracy: 0.4036 - weight_output_accuracy: 0.6377 - bag_output_accuracy: 0.6293 - footwear_output_accuracy: 0.6633 - pose_output_accuracy: 0.7744 - emotion_output_accuracy: 0.7124 - val_loss: 6.6609 - val_gender_output_loss: 0.4008 - val_image_quality_output_loss: 0.9123 - val_age_output_loss: 1.3730 - val_weight_output_loss: 0.9319 - val_bag_output_loss: 0.8381 - val_footwear_output_loss: 0.7581 - val_pose_output_loss: 0.5605 - val_emotion_output_loss: 0.8862 - val_gender_output_accuracy: 0.8145 - val_image_quality_output_accuracy: 0.5590 - val_age_output_accuracy: 0.4018 - val_weight_output_accuracy: 0.6463 - val_bag_output_accuracy: 0.6210 - val_footwear_output_accuracy: 0.6682 - val_pose_output_accuracy: 0.7728 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 6.66088\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 6.5591 - gender_output_loss: 0.3985 - image_quality_output_loss: 0.9027 - age_output_loss: 1.3518 - weight_output_loss: 0.9265 - bag_output_loss: 0.8198 - footwear_output_loss: 0.7648 - pose_output_loss: 0.5290 - emotion_output_loss: 0.8660 - gender_output_accuracy: 0.8184 - image_quality_output_accuracy: 0.5672 - age_output_accuracy: 0.4097 - weight_output_accuracy: 0.6403 - bag_output_accuracy: 0.6397 - footwear_output_accuracy: 0.6641 - pose_output_accuracy: 0.7862 - emotion_output_accuracy: 0.7118 - val_loss: 6.6778 - val_gender_output_loss: 0.3857 - val_image_quality_output_loss: 0.9395 - val_age_output_loss: 1.3683 - val_weight_output_loss: 0.9305 - val_bag_output_loss: 0.8452 - val_footwear_output_loss: 0.7666 - val_pose_output_loss: 0.5425 - val_emotion_output_loss: 0.8996 - val_gender_output_accuracy: 0.8333 - val_image_quality_output_accuracy: 0.5491 - val_age_output_accuracy: 0.3993 - val_weight_output_accuracy: 0.6443 - val_bag_output_accuracy: 0.6344 - val_footwear_output_accuracy: 0.6632 - val_pose_output_accuracy: 0.7842 - val_emotion_output_accuracy: 0.7039\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 00020: val_loss improved from 6.66088 to 6.62524, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.4707 - gender_output_loss: 0.3755 - image_quality_output_loss: 0.8943 - age_output_loss: 1.3396 - weight_output_loss: 0.9235 - bag_output_loss: 0.8116 - footwear_output_loss: 0.7580 - pose_output_loss: 0.5069 - emotion_output_loss: 0.8612 - gender_output_accuracy: 0.8316 - image_quality_output_accuracy: 0.5711 - age_output_accuracy: 0.4089 - weight_output_accuracy: 0.6405 - bag_output_accuracy: 0.6457 - footwear_output_accuracy: 0.6688 - pose_output_accuracy: 0.7968 - emotion_output_accuracy: 0.7131 - val_loss: 6.6252 - val_gender_output_loss: 0.3925 - val_image_quality_output_loss: 0.9077 - val_age_output_loss: 1.3720 - val_weight_output_loss: 0.9205 - val_bag_output_loss: 0.8399 - val_footwear_output_loss: 0.7731 - val_pose_output_loss: 0.5343 - val_emotion_output_loss: 0.8852 - val_gender_output_accuracy: 0.8264 - val_image_quality_output_accuracy: 0.5689 - val_age_output_accuracy: 0.3924 - val_weight_output_accuracy: 0.6478 - val_bag_output_accuracy: 0.6339 - val_footwear_output_accuracy: 0.6562 - val_pose_output_accuracy: 0.7877 - val_emotion_output_accuracy: 0.7034\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.62524 to 6.55160, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.3882 - gender_output_loss: 0.3597 - image_quality_output_loss: 0.8873 - age_output_loss: 1.3268 - weight_output_loss: 0.9182 - bag_output_loss: 0.8011 - footwear_output_loss: 0.7509 - pose_output_loss: 0.4885 - emotion_output_loss: 0.8557 - gender_output_accuracy: 0.8398 - image_quality_output_accuracy: 0.5773 - age_output_accuracy: 0.4234 - weight_output_accuracy: 0.6423 - bag_output_accuracy: 0.6524 - footwear_output_accuracy: 0.6739 - pose_output_accuracy: 0.8035 - emotion_output_accuracy: 0.7134 - val_loss: 6.5516 - val_gender_output_loss: 0.3691 - val_image_quality_output_loss: 0.9023 - val_age_output_loss: 1.3555 - val_weight_output_loss: 0.9225 - val_bag_output_loss: 0.8195 - val_footwear_output_loss: 0.7744 - val_pose_output_loss: 0.5120 - val_emotion_output_loss: 0.8964 - val_gender_output_accuracy: 0.8373 - val_image_quality_output_accuracy: 0.5670 - val_age_output_accuracy: 0.3968 - val_weight_output_accuracy: 0.6453 - val_bag_output_accuracy: 0.6310 - val_footwear_output_accuracy: 0.6696 - val_pose_output_accuracy: 0.7961 - val_emotion_output_accuracy: 0.7024\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 6.55160\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 6.2915 - gender_output_loss: 0.3410 - image_quality_output_loss: 0.8809 - age_output_loss: 1.3116 - weight_output_loss: 0.9087 - bag_output_loss: 0.7890 - footwear_output_loss: 0.7398 - pose_output_loss: 0.4702 - emotion_output_loss: 0.8504 - gender_output_accuracy: 0.8496 - image_quality_output_accuracy: 0.5763 - age_output_accuracy: 0.4273 - weight_output_accuracy: 0.6431 - bag_output_accuracy: 0.6633 - footwear_output_accuracy: 0.6798 - pose_output_accuracy: 0.8152 - emotion_output_accuracy: 0.7120 - val_loss: 6.5573 - val_gender_output_loss: 0.3598 - val_image_quality_output_loss: 0.9473 - val_age_output_loss: 1.3598 - val_weight_output_loss: 0.9212 - val_bag_output_loss: 0.8215 - val_footwear_output_loss: 0.7502 - val_pose_output_loss: 0.5010 - val_emotion_output_loss: 0.8965 - val_gender_output_accuracy: 0.8408 - val_image_quality_output_accuracy: 0.5446 - val_age_output_accuracy: 0.4008 - val_weight_output_accuracy: 0.6438 - val_bag_output_accuracy: 0.6339 - val_footwear_output_accuracy: 0.6736 - val_pose_output_accuracy: 0.8011 - val_emotion_output_accuracy: 0.7019\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 00023: val_loss improved from 6.55160 to 6.53573, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 88s - loss: 6.2175 - gender_output_loss: 0.3319 - image_quality_output_loss: 0.8700 - age_output_loss: 1.2967 - weight_output_loss: 0.8962 - bag_output_loss: 0.7818 - footwear_output_loss: 0.7372 - pose_output_loss: 0.4551 - emotion_output_loss: 0.8486 - gender_output_accuracy: 0.8558 - image_quality_output_accuracy: 0.5815 - age_output_accuracy: 0.4313 - weight_output_accuracy: 0.6464 - bag_output_accuracy: 0.6651 - footwear_output_accuracy: 0.6837 - pose_output_accuracy: 0.8194 - emotion_output_accuracy: 0.7122 - val_loss: 6.5357 - val_gender_output_loss: 0.3742 - val_image_quality_output_loss: 0.9074 - val_age_output_loss: 1.3636 - val_weight_output_loss: 0.9204 - val_bag_output_loss: 0.8152 - val_footwear_output_loss: 0.7529 - val_pose_output_loss: 0.5076 - val_emotion_output_loss: 0.8944 - val_gender_output_accuracy: 0.8428 - val_image_quality_output_accuracy: 0.5709 - val_age_output_accuracy: 0.4053 - val_weight_output_accuracy: 0.6468 - val_bag_output_accuracy: 0.6503 - val_footwear_output_accuracy: 0.6642 - val_pose_output_accuracy: 0.8026 - val_emotion_output_accuracy: 0.7004\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 6.53573\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 6.1067 - gender_output_loss: 0.3125 - image_quality_output_loss: 0.8656 - age_output_loss: 1.2791 - weight_output_loss: 0.8862 - bag_output_loss: 0.7652 - footwear_output_loss: 0.7243 - pose_output_loss: 0.4307 - emotion_output_loss: 0.8432 - gender_output_accuracy: 0.8689 - image_quality_output_accuracy: 0.5849 - age_output_accuracy: 0.4369 - weight_output_accuracy: 0.6491 - bag_output_accuracy: 0.6691 - footwear_output_accuracy: 0.6869 - pose_output_accuracy: 0.8319 - emotion_output_accuracy: 0.7132 - val_loss: 6.6365 - val_gender_output_loss: 0.4674 - val_image_quality_output_loss: 0.8965 - val_age_output_loss: 1.3456 - val_weight_output_loss: 0.9161 - val_bag_output_loss: 0.8455 - val_footwear_output_loss: 0.7772 - val_pose_output_loss: 0.4999 - val_emotion_output_loss: 0.8882 - val_gender_output_accuracy: 0.8061 - val_image_quality_output_accuracy: 0.5714 - val_age_output_accuracy: 0.4127 - val_weight_output_accuracy: 0.6414 - val_bag_output_accuracy: 0.6215 - val_footwear_output_accuracy: 0.6657 - val_pose_output_accuracy: 0.7986 - val_emotion_output_accuracy: 0.7019\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 00025: val_loss improved from 6.53573 to 6.52816, saving model to /content/gdrive/My Drive/EIP4/models/PersonAttributes_VGG16_model.h5\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 89s - loss: 6.0007 - gender_output_loss: 0.2977 - image_quality_output_loss: 0.8570 - age_output_loss: 1.2583 - weight_output_loss: 0.8772 - bag_output_loss: 0.7501 - footwear_output_loss: 0.7077 - pose_output_loss: 0.4135 - emotion_output_loss: 0.8391 - gender_output_accuracy: 0.8725 - image_quality_output_accuracy: 0.5880 - age_output_accuracy: 0.4492 - weight_output_accuracy: 0.6512 - bag_output_accuracy: 0.6815 - footwear_output_accuracy: 0.6963 - pose_output_accuracy: 0.8398 - emotion_output_accuracy: 0.7142 - val_loss: 6.5282 - val_gender_output_loss: 0.3410 - val_image_quality_output_loss: 0.9302 - val_age_output_loss: 1.3479 - val_weight_output_loss: 0.9149 - val_bag_output_loss: 0.8234 - val_footwear_output_loss: 0.7757 - val_pose_output_loss: 0.5017 - val_emotion_output_loss: 0.8934 - val_gender_output_accuracy: 0.8537 - val_image_quality_output_accuracy: 0.5635 - val_age_output_accuracy: 0.4062 - val_weight_output_accuracy: 0.6334 - val_bag_output_accuracy: 0.6404 - val_footwear_output_accuracy: 0.6682 - val_pose_output_accuracy: 0.8036 - val_emotion_output_accuracy: 0.7049\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 5.9151 - gender_output_loss: 0.2815 - image_quality_output_loss: 0.8521 - age_output_loss: 1.2485 - weight_output_loss: 0.8614 - bag_output_loss: 0.7429 - footwear_output_loss: 0.7085 - pose_output_loss: 0.3910 - emotion_output_loss: 0.8291 - gender_output_accuracy: 0.8816 - image_quality_output_accuracy: 0.5918 - age_output_accuracy: 0.4511 - weight_output_accuracy: 0.6573 - bag_output_accuracy: 0.6897 - footwear_output_accuracy: 0.6914 - pose_output_accuracy: 0.8474 - emotion_output_accuracy: 0.7128 - val_loss: 6.5684 - val_gender_output_loss: 0.3732 - val_image_quality_output_loss: 0.9128 - val_age_output_loss: 1.3595 - val_weight_output_loss: 0.9151 - val_bag_output_loss: 0.8228 - val_footwear_output_loss: 0.7712 - val_pose_output_loss: 0.5166 - val_emotion_output_loss: 0.8972 - val_gender_output_accuracy: 0.8398 - val_image_quality_output_accuracy: 0.5625 - val_age_output_accuracy: 0.4043 - val_weight_output_accuracy: 0.6354 - val_bag_output_accuracy: 0.6389 - val_footwear_output_accuracy: 0.6716 - val_pose_output_accuracy: 0.7976 - val_emotion_output_accuracy: 0.7024\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.0010000000474974513\n",
            "360/360 - 87s - loss: 5.8131 - gender_output_loss: 0.2784 - image_quality_output_loss: 0.8463 - age_output_loss: 1.2278 - weight_output_loss: 0.8413 - bag_output_loss: 0.7218 - footwear_output_loss: 0.6960 - pose_output_loss: 0.3744 - emotion_output_loss: 0.8272 - gender_output_accuracy: 0.8830 - image_quality_output_accuracy: 0.6006 - age_output_accuracy: 0.4624 - weight_output_accuracy: 0.6626 - bag_output_accuracy: 0.6948 - footwear_output_accuracy: 0.6947 - pose_output_accuracy: 0.8608 - emotion_output_accuracy: 0.7141 - val_loss: 6.7016 - val_gender_output_loss: 0.3788 - val_image_quality_output_loss: 0.9961 - val_age_output_loss: 1.3734 - val_weight_output_loss: 0.9398 - val_bag_output_loss: 0.8268 - val_footwear_output_loss: 0.7845 - val_pose_output_loss: 0.4897 - val_emotion_output_loss: 0.9124 - val_gender_output_accuracy: 0.8393 - val_image_quality_output_accuracy: 0.5084 - val_age_output_accuracy: 0.4028 - val_weight_output_accuracy: 0.6314 - val_bag_output_accuracy: 0.6429 - val_footwear_output_accuracy: 0.6637 - val_pose_output_accuracy: 0.8160 - val_emotion_output_accuracy: 0.6935\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.0003162277862429619\n",
            "360/360 - 87s - loss: 5.6859 - gender_output_loss: 0.2508 - image_quality_output_loss: 0.8311 - age_output_loss: 1.2093 - weight_output_loss: 0.8265 - bag_output_loss: 0.7043 - footwear_output_loss: 0.6836 - pose_output_loss: 0.3650 - emotion_output_loss: 0.8152 - gender_output_accuracy: 0.8957 - image_quality_output_accuracy: 0.6056 - age_output_accuracy: 0.4677 - weight_output_accuracy: 0.6687 - bag_output_accuracy: 0.7041 - footwear_output_accuracy: 0.7023 - pose_output_accuracy: 0.8602 - emotion_output_accuracy: 0.7151 - val_loss: 6.6479 - val_gender_output_loss: 0.3453 - val_image_quality_output_loss: 0.9061 - val_age_output_loss: 1.3924 - val_weight_output_loss: 0.9566 - val_bag_output_loss: 0.8192 - val_footwear_output_loss: 0.7783 - val_pose_output_loss: 0.5397 - val_emotion_output_loss: 0.9102 - val_gender_output_accuracy: 0.8586 - val_image_quality_output_accuracy: 0.5670 - val_age_output_accuracy: 0.3849 - val_weight_output_accuracy: 0.6354 - val_bag_output_accuracy: 0.6572 - val_footwear_output_accuracy: 0.6637 - val_pose_output_accuracy: 0.7847 - val_emotion_output_accuracy: 0.7014\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.0003162277862429619\n",
            "360/360 - 87s - loss: 5.0675 - gender_output_loss: 0.1892 - image_quality_output_loss: 0.7750 - age_output_loss: 1.1070 - weight_output_loss: 0.7433 - bag_output_loss: 0.6104 - footwear_output_loss: 0.6124 - pose_output_loss: 0.2643 - emotion_output_loss: 0.7660 - gender_output_accuracy: 0.9274 - image_quality_output_accuracy: 0.6413 - age_output_accuracy: 0.5206 - weight_output_accuracy: 0.6989 - bag_output_accuracy: 0.7553 - footwear_output_accuracy: 0.7382 - pose_output_accuracy: 0.9091 - emotion_output_accuracy: 0.7214 - val_loss: 6.7372 - val_gender_output_loss: 0.3986 - val_image_quality_output_loss: 0.9382 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9439 - val_bag_output_loss: 0.8499 - val_footwear_output_loss: 0.7970 - val_pose_output_loss: 0.4941 - val_emotion_output_loss: 0.9253 - val_gender_output_accuracy: 0.8428 - val_image_quality_output_accuracy: 0.5526 - val_age_output_accuracy: 0.3983 - val_weight_output_accuracy: 0.6225 - val_bag_output_accuracy: 0.6374 - val_footwear_output_accuracy: 0.6538 - val_pose_output_accuracy: 0.8214 - val_emotion_output_accuracy: 0.6984\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.0003162277862429619\n",
            "360/360 - 87s - loss: 4.6603 - gender_output_loss: 0.1444 - image_quality_output_loss: 0.7348 - age_output_loss: 1.0414 - weight_output_loss: 0.6758 - bag_output_loss: 0.5507 - footwear_output_loss: 0.5621 - pose_output_loss: 0.2171 - emotion_output_loss: 0.7340 - gender_output_accuracy: 0.9470 - image_quality_output_accuracy: 0.6668 - age_output_accuracy: 0.5564 - weight_output_accuracy: 0.7306 - bag_output_accuracy: 0.7845 - footwear_output_accuracy: 0.7668 - pose_output_accuracy: 0.9261 - emotion_output_accuracy: 0.7341 - val_loss: 7.0868 - val_gender_output_loss: 0.4773 - val_image_quality_output_loss: 0.9598 - val_age_output_loss: 1.4237 - val_weight_output_loss: 0.9940 - val_bag_output_loss: 0.8971 - val_footwear_output_loss: 0.8490 - val_pose_output_loss: 0.5365 - val_emotion_output_loss: 0.9495 - val_gender_output_accuracy: 0.8199 - val_image_quality_output_accuracy: 0.5397 - val_age_output_accuracy: 0.4043 - val_weight_output_accuracy: 0.6106 - val_bag_output_accuracy: 0.6156 - val_footwear_output_accuracy: 0.6463 - val_pose_output_accuracy: 0.8095 - val_emotion_output_accuracy: 0.6801\n",
            "Epoch 31/200\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.00010000000474974513\n",
            "360/360 - 87s - loss: 4.3609 - gender_output_loss: 0.1341 - image_quality_output_loss: 0.6995 - age_output_loss: 0.9877 - weight_output_loss: 0.6346 - bag_output_loss: 0.4931 - footwear_output_loss: 0.5195 - pose_output_loss: 0.1876 - emotion_output_loss: 0.7049 - gender_output_accuracy: 0.9529 - image_quality_output_accuracy: 0.6885 - age_output_accuracy: 0.5861 - weight_output_accuracy: 0.7488 - bag_output_accuracy: 0.8071 - footwear_output_accuracy: 0.7798 - pose_output_accuracy: 0.9368 - emotion_output_accuracy: 0.7444 - val_loss: 7.5391 - val_gender_output_loss: 0.4412 - val_image_quality_output_loss: 1.0283 - val_age_output_loss: 1.4689 - val_weight_output_loss: 1.0757 - val_bag_output_loss: 1.0769 - val_footwear_output_loss: 0.9230 - val_pose_output_loss: 0.5449 - val_emotion_output_loss: 0.9802 - val_gender_output_accuracy: 0.8418 - val_image_quality_output_accuracy: 0.5263 - val_age_output_accuracy: 0.3998 - val_weight_output_accuracy: 0.5769 - val_bag_output_accuracy: 0.5878 - val_footwear_output_accuracy: 0.6270 - val_pose_output_accuracy: 0.8180 - val_emotion_output_accuracy: 0.6637\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.00010000000474974513\n",
            "360/360 - 87s - loss: 3.7995 - gender_output_loss: 0.1033 - image_quality_output_loss: 0.6232 - age_output_loss: 0.8931 - weight_output_loss: 0.5470 - bag_output_loss: 0.3995 - footwear_output_loss: 0.4488 - pose_output_loss: 0.1470 - emotion_output_loss: 0.6378 - gender_output_accuracy: 0.9647 - image_quality_output_accuracy: 0.7374 - age_output_accuracy: 0.6418 - weight_output_accuracy: 0.7933 - bag_output_accuracy: 0.8576 - footwear_output_accuracy: 0.8186 - pose_output_accuracy: 0.9571 - emotion_output_accuracy: 0.7655 - val_loss: 7.2655 - val_gender_output_loss: 0.3851 - val_image_quality_output_loss: 1.0085 - val_age_output_loss: 1.4834 - val_weight_output_loss: 1.0400 - val_bag_output_loss: 0.9375 - val_footwear_output_loss: 0.8985 - val_pose_output_loss: 0.5355 - val_emotion_output_loss: 0.9770 - val_gender_output_accuracy: 0.8646 - val_image_quality_output_accuracy: 0.5263 - val_age_output_accuracy: 0.3889 - val_weight_output_accuracy: 0.6215 - val_bag_output_accuracy: 0.6354 - val_footwear_output_accuracy: 0.6389 - val_pose_output_accuracy: 0.8130 - val_emotion_output_accuracy: 0.6815\n",
            "Epoch 33/200\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 0.00010000000474974513\n",
            "360/360 - 87s - loss: 3.4439 - gender_output_loss: 0.0833 - image_quality_output_loss: 0.5711 - age_output_loss: 0.8329 - weight_output_loss: 0.4907 - bag_output_loss: 0.3377 - footwear_output_loss: 0.4063 - pose_output_loss: 0.1234 - emotion_output_loss: 0.5985 - gender_output_accuracy: 0.9740 - image_quality_output_accuracy: 0.7691 - age_output_accuracy: 0.6740 - weight_output_accuracy: 0.8215 - bag_output_accuracy: 0.8851 - footwear_output_accuracy: 0.8420 - pose_output_accuracy: 0.9650 - emotion_output_accuracy: 0.7848 - val_loss: 7.5340 - val_gender_output_loss: 0.4332 - val_image_quality_output_loss: 1.0713 - val_age_output_loss: 1.5066 - val_weight_output_loss: 1.0540 - val_bag_output_loss: 0.9861 - val_footwear_output_loss: 0.9216 - val_pose_output_loss: 0.5466 - val_emotion_output_loss: 1.0145 - val_gender_output_accuracy: 0.8418 - val_image_quality_output_accuracy: 0.5010 - val_age_output_accuracy: 0.3839 - val_weight_output_accuracy: 0.6176 - val_bag_output_accuracy: 0.6265 - val_footwear_output_accuracy: 0.6275 - val_pose_output_accuracy: 0.8140 - val_emotion_output_accuracy: 0.6458\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162277789670043e-05\n",
            "360/360 - 87s - loss: 3.2003 - gender_output_loss: 0.0762 - image_quality_output_loss: 0.5271 - age_output_loss: 0.7926 - weight_output_loss: 0.4562 - bag_output_loss: 0.2984 - footwear_output_loss: 0.3749 - pose_output_loss: 0.1125 - emotion_output_loss: 0.5626 - gender_output_accuracy: 0.9771 - image_quality_output_accuracy: 0.7977 - age_output_accuracy: 0.6990 - weight_output_accuracy: 0.8366 - bag_output_accuracy: 0.9030 - footwear_output_accuracy: 0.8580 - pose_output_accuracy: 0.9700 - emotion_output_accuracy: 0.8011 - val_loss: 7.6576 - val_gender_output_loss: 0.4122 - val_image_quality_output_loss: 1.0464 - val_age_output_loss: 1.5048 - val_weight_output_loss: 1.0849 - val_bag_output_loss: 1.0199 - val_footwear_output_loss: 0.9757 - val_pose_output_loss: 0.6020 - val_emotion_output_loss: 1.0116 - val_gender_output_accuracy: 0.8522 - val_image_quality_output_accuracy: 0.5248 - val_age_output_accuracy: 0.3775 - val_weight_output_accuracy: 0.5982 - val_bag_output_accuracy: 0.6300 - val_footwear_output_accuracy: 0.6270 - val_pose_output_accuracy: 0.7961 - val_emotion_output_accuracy: 0.6463\n",
            "Epoch 35/200\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162277789670043e-05\n",
            "360/360 - 87s - loss: 2.8906 - gender_output_loss: 0.0656 - image_quality_output_loss: 0.4744 - age_output_loss: 0.7304 - weight_output_loss: 0.4103 - bag_output_loss: 0.2574 - footwear_output_loss: 0.3339 - pose_output_loss: 0.0983 - emotion_output_loss: 0.5204 - gender_output_accuracy: 0.9800 - image_quality_output_accuracy: 0.8363 - age_output_accuracy: 0.7324 - weight_output_accuracy: 0.8618 - bag_output_accuracy: 0.9191 - footwear_output_accuracy: 0.8797 - pose_output_accuracy: 0.9776 - emotion_output_accuracy: 0.8220 - val_loss: 7.6929 - val_gender_output_loss: 0.4090 - val_image_quality_output_loss: 1.0844 - val_age_output_loss: 1.5282 - val_weight_output_loss: 1.1226 - val_bag_output_loss: 0.9953 - val_footwear_output_loss: 0.9474 - val_pose_output_loss: 0.5730 - val_emotion_output_loss: 1.0329 - val_gender_output_accuracy: 0.8512 - val_image_quality_output_accuracy: 0.5060 - val_age_output_accuracy: 0.3919 - val_weight_output_accuracy: 0.5784 - val_bag_output_accuracy: 0.6265 - val_footwear_output_accuracy: 0.6260 - val_pose_output_accuracy: 0.8056 - val_emotion_output_accuracy: 0.6806\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162277789670043e-05\n",
            "360/360 - 87s - loss: 2.7464 - gender_output_loss: 0.0600 - image_quality_output_loss: 0.4465 - age_output_loss: 0.7084 - weight_output_loss: 0.3899 - bag_output_loss: 0.2357 - footwear_output_loss: 0.3162 - pose_output_loss: 0.0962 - emotion_output_loss: 0.4934 - gender_output_accuracy: 0.9832 - image_quality_output_accuracy: 0.8500 - age_output_accuracy: 0.7447 - weight_output_accuracy: 0.8705 - bag_output_accuracy: 0.9316 - footwear_output_accuracy: 0.8872 - pose_output_accuracy: 0.9770 - emotion_output_accuracy: 0.8365 - val_loss: 7.8528 - val_gender_output_loss: 0.4369 - val_image_quality_output_loss: 1.0986 - val_age_output_loss: 1.5384 - val_weight_output_loss: 1.1224 - val_bag_output_loss: 1.0786 - val_footwear_output_loss: 0.9774 - val_pose_output_loss: 0.5613 - val_emotion_output_loss: 1.0392 - val_gender_output_accuracy: 0.8408 - val_image_quality_output_accuracy: 0.5064 - val_age_output_accuracy: 0.3869 - val_weight_output_accuracy: 0.5997 - val_bag_output_accuracy: 0.6220 - val_footwear_output_accuracy: 0.6205 - val_pose_output_accuracy: 0.8135 - val_emotion_output_accuracy: 0.6652\n",
            "Epoch 37/200\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000000656873453e-05\n",
            "360/360 - 87s - loss: 2.6434 - gender_output_loss: 0.0581 - image_quality_output_loss: 0.4332 - age_output_loss: 0.6843 - weight_output_loss: 0.3709 - bag_output_loss: 0.2189 - footwear_output_loss: 0.3072 - pose_output_loss: 0.0893 - emotion_output_loss: 0.4815 - gender_output_accuracy: 0.9850 - image_quality_output_accuracy: 0.8602 - age_output_accuracy: 0.7594 - weight_output_accuracy: 0.8795 - bag_output_accuracy: 0.9400 - footwear_output_accuracy: 0.8913 - pose_output_accuracy: 0.9787 - emotion_output_accuracy: 0.8394 - val_loss: 7.8899 - val_gender_output_loss: 0.4256 - val_image_quality_output_loss: 1.1314 - val_age_output_loss: 1.5711 - val_weight_output_loss: 1.1171 - val_bag_output_loss: 1.0314 - val_footwear_output_loss: 0.9727 - val_pose_output_loss: 0.5828 - val_emotion_output_loss: 1.0578 - val_gender_output_accuracy: 0.8497 - val_image_quality_output_accuracy: 0.5124 - val_age_output_accuracy: 0.3745 - val_weight_output_accuracy: 0.6195 - val_bag_output_accuracy: 0.6260 - val_footwear_output_accuracy: 0.6275 - val_pose_output_accuracy: 0.8115 - val_emotion_output_accuracy: 0.6687\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000000656873453e-05\n",
            "360/360 - 87s - loss: 2.5453 - gender_output_loss: 0.0568 - image_quality_output_loss: 0.4105 - age_output_loss: 0.6680 - weight_output_loss: 0.3589 - bag_output_loss: 0.2081 - footwear_output_loss: 0.2918 - pose_output_loss: 0.0841 - emotion_output_loss: 0.4670 - gender_output_accuracy: 0.9847 - image_quality_output_accuracy: 0.8724 - age_output_accuracy: 0.7741 - weight_output_accuracy: 0.8896 - bag_output_accuracy: 0.9438 - footwear_output_accuracy: 0.8998 - pose_output_accuracy: 0.9830 - emotion_output_accuracy: 0.8460 - val_loss: 7.8159 - val_gender_output_loss: 0.4193 - val_image_quality_output_loss: 1.1022 - val_age_output_loss: 1.5657 - val_weight_output_loss: 1.0998 - val_bag_output_loss: 1.0071 - val_footwear_output_loss: 0.9685 - val_pose_output_loss: 0.6031 - val_emotion_output_loss: 1.0502 - val_gender_output_accuracy: 0.8457 - val_image_quality_output_accuracy: 0.5104 - val_age_output_accuracy: 0.3705 - val_weight_output_accuracy: 0.6101 - val_bag_output_accuracy: 0.6414 - val_footwear_output_accuracy: 0.6190 - val_pose_output_accuracy: 0.8011 - val_emotion_output_accuracy: 0.6344\n",
            "Epoch 39/200\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000000656873453e-05\n",
            "360/360 - 87s - loss: 2.5182 - gender_output_loss: 0.0596 - image_quality_output_loss: 0.4084 - age_output_loss: 0.6591 - weight_output_loss: 0.3518 - bag_output_loss: 0.2070 - footwear_output_loss: 0.2855 - pose_output_loss: 0.0820 - emotion_output_loss: 0.4647 - gender_output_accuracy: 0.9859 - image_quality_output_accuracy: 0.8709 - age_output_accuracy: 0.7727 - weight_output_accuracy: 0.8900 - bag_output_accuracy: 0.9434 - footwear_output_accuracy: 0.9026 - pose_output_accuracy: 0.9821 - emotion_output_accuracy: 0.8455 - val_loss: 8.0952 - val_gender_output_loss: 0.4501 - val_image_quality_output_loss: 1.1265 - val_age_output_loss: 1.5916 - val_weight_output_loss: 1.1431 - val_bag_output_loss: 1.0466 - val_footwear_output_loss: 1.0421 - val_pose_output_loss: 0.6175 - val_emotion_output_loss: 1.0777 - val_gender_output_accuracy: 0.8482 - val_image_quality_output_accuracy: 0.5144 - val_age_output_accuracy: 0.3735 - val_weight_output_accuracy: 0.5898 - val_bag_output_accuracy: 0.6424 - val_footwear_output_accuracy: 0.6181 - val_pose_output_accuracy: 0.8021 - val_emotion_output_accuracy: 0.6756\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.1622778351447778e-06\n",
            "360/360 - 87s - loss: 2.4681 - gender_output_loss: 0.0535 - image_quality_output_loss: 0.3973 - age_output_loss: 0.6526 - weight_output_loss: 0.3448 - bag_output_loss: 0.2011 - footwear_output_loss: 0.2842 - pose_output_loss: 0.0809 - emotion_output_loss: 0.4536 - gender_output_accuracy: 0.9870 - image_quality_output_accuracy: 0.8773 - age_output_accuracy: 0.7787 - weight_output_accuracy: 0.8936 - bag_output_accuracy: 0.9451 - footwear_output_accuracy: 0.9005 - pose_output_accuracy: 0.9830 - emotion_output_accuracy: 0.8538 - val_loss: 8.0294 - val_gender_output_loss: 0.4375 - val_image_quality_output_loss: 1.1254 - val_age_output_loss: 1.5774 - val_weight_output_loss: 1.1633 - val_bag_output_loss: 1.0623 - val_footwear_output_loss: 1.0109 - val_pose_output_loss: 0.5891 - val_emotion_output_loss: 1.0635 - val_gender_output_accuracy: 0.8512 - val_image_quality_output_accuracy: 0.4955 - val_age_output_accuracy: 0.3790 - val_weight_output_accuracy: 0.5794 - val_bag_output_accuracy: 0.6280 - val_footwear_output_accuracy: 0.6151 - val_pose_output_accuracy: 0.8095 - val_emotion_output_accuracy: 0.6468\n",
            "Epoch 41/200\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.1622778351447778e-06\n",
            "360/360 - 87s - loss: 2.4324 - gender_output_loss: 0.0574 - image_quality_output_loss: 0.3895 - age_output_loss: 0.6454 - weight_output_loss: 0.3437 - bag_output_loss: 0.1922 - footwear_output_loss: 0.2784 - pose_output_loss: 0.0823 - emotion_output_loss: 0.4436 - gender_output_accuracy: 0.9858 - image_quality_output_accuracy: 0.8835 - age_output_accuracy: 0.7779 - weight_output_accuracy: 0.8933 - bag_output_accuracy: 0.9514 - footwear_output_accuracy: 0.9069 - pose_output_accuracy: 0.9827 - emotion_output_accuracy: 0.8584 - val_loss: 8.0654 - val_gender_output_loss: 0.4330 - val_image_quality_output_loss: 1.1188 - val_age_output_loss: 1.5917 - val_weight_output_loss: 1.1415 - val_bag_output_loss: 1.0898 - val_footwear_output_loss: 1.0217 - val_pose_output_loss: 0.5919 - val_emotion_output_loss: 1.0770 - val_gender_output_accuracy: 0.8467 - val_image_quality_output_accuracy: 0.5005 - val_age_output_accuracy: 0.3760 - val_weight_output_accuracy: 0.6012 - val_bag_output_accuracy: 0.6121 - val_footwear_output_accuracy: 0.6141 - val_pose_output_accuracy: 0.8085 - val_emotion_output_accuracy: 0.6240\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.1622778351447778e-06\n",
            "360/360 - 87s - loss: 2.4132 - gender_output_loss: 0.0519 - image_quality_output_loss: 0.3838 - age_output_loss: 0.6385 - weight_output_loss: 0.3398 - bag_output_loss: 0.1916 - footwear_output_loss: 0.2770 - pose_output_loss: 0.0801 - emotion_output_loss: 0.4505 - gender_output_accuracy: 0.9889 - image_quality_output_accuracy: 0.8851 - age_output_accuracy: 0.7860 - weight_output_accuracy: 0.9002 - bag_output_accuracy: 0.9514 - footwear_output_accuracy: 0.9085 - pose_output_accuracy: 0.9841 - emotion_output_accuracy: 0.8524 - val_loss: 8.1908 - val_gender_output_loss: 0.4429 - val_image_quality_output_loss: 1.1434 - val_age_output_loss: 1.5853 - val_weight_output_loss: 1.1537 - val_bag_output_loss: 1.1035 - val_footwear_output_loss: 1.0889 - val_pose_output_loss: 0.5941 - val_emotion_output_loss: 1.0789 - val_gender_output_accuracy: 0.8428 - val_image_quality_output_accuracy: 0.5124 - val_age_output_accuracy: 0.3800 - val_weight_output_accuracy: 0.5898 - val_bag_output_accuracy: 0.6091 - val_footwear_output_accuracy: 0.6096 - val_pose_output_accuracy: 0.8056 - val_emotion_output_accuracy: 0.6572\n",
            "Epoch 43/200\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001111620804e-06\n",
            "360/360 - 87s - loss: 2.3938 - gender_output_loss: 0.0513 - image_quality_output_loss: 0.3821 - age_output_loss: 0.6381 - weight_output_loss: 0.3360 - bag_output_loss: 0.1913 - footwear_output_loss: 0.2757 - pose_output_loss: 0.0794 - emotion_output_loss: 0.4398 - gender_output_accuracy: 0.9873 - image_quality_output_accuracy: 0.8900 - age_output_accuracy: 0.7839 - weight_output_accuracy: 0.8995 - bag_output_accuracy: 0.9504 - footwear_output_accuracy: 0.9062 - pose_output_accuracy: 0.9847 - emotion_output_accuracy: 0.8603 - val_loss: 8.1982 - val_gender_output_loss: 0.4705 - val_image_quality_output_loss: 1.1293 - val_age_output_loss: 1.6052 - val_weight_output_loss: 1.1567 - val_bag_output_loss: 1.1108 - val_footwear_output_loss: 1.0116 - val_pose_output_loss: 0.6402 - val_emotion_output_loss: 1.0740 - val_gender_output_accuracy: 0.8328 - val_image_quality_output_accuracy: 0.5144 - val_age_output_accuracy: 0.3775 - val_weight_output_accuracy: 0.6027 - val_bag_output_accuracy: 0.5982 - val_footwear_output_accuracy: 0.6131 - val_pose_output_accuracy: 0.8051 - val_emotion_output_accuracy: 0.6151\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001111620804e-06\n",
            "360/360 - 87s - loss: 2.3835 - gender_output_loss: 0.0581 - image_quality_output_loss: 0.3780 - age_output_loss: 0.6376 - weight_output_loss: 0.3378 - bag_output_loss: 0.1899 - footwear_output_loss: 0.2704 - pose_output_loss: 0.0746 - emotion_output_loss: 0.4372 - gender_output_accuracy: 0.9850 - image_quality_output_accuracy: 0.8899 - age_output_accuracy: 0.7876 - weight_output_accuracy: 0.8977 - bag_output_accuracy: 0.9510 - footwear_output_accuracy: 0.9112 - pose_output_accuracy: 0.9867 - emotion_output_accuracy: 0.8615 - val_loss: 8.0335 - val_gender_output_loss: 0.4397 - val_image_quality_output_loss: 1.1206 - val_age_output_loss: 1.5896 - val_weight_output_loss: 1.1714 - val_bag_output_loss: 1.0530 - val_footwear_output_loss: 1.0047 - val_pose_output_loss: 0.5903 - val_emotion_output_loss: 1.0642 - val_gender_output_accuracy: 0.8438 - val_image_quality_output_accuracy: 0.5149 - val_age_output_accuracy: 0.3834 - val_weight_output_accuracy: 0.5799 - val_bag_output_accuracy: 0.6270 - val_footwear_output_accuracy: 0.6121 - val_pose_output_accuracy: 0.8095 - val_emotion_output_accuracy: 0.6662\n",
            "Epoch 45/200\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001111620804e-06\n",
            "360/360 - 87s - loss: 2.3803 - gender_output_loss: 0.0509 - image_quality_output_loss: 0.3794 - age_output_loss: 0.6394 - weight_output_loss: 0.3333 - bag_output_loss: 0.1851 - footwear_output_loss: 0.2742 - pose_output_loss: 0.0774 - emotion_output_loss: 0.4406 - gender_output_accuracy: 0.9882 - image_quality_output_accuracy: 0.8872 - age_output_accuracy: 0.7818 - weight_output_accuracy: 0.8966 - bag_output_accuracy: 0.9534 - footwear_output_accuracy: 0.9109 - pose_output_accuracy: 0.9848 - emotion_output_accuracy: 0.8594 - val_loss: 8.1286 - val_gender_output_loss: 0.4429 - val_image_quality_output_loss: 1.1425 - val_age_output_loss: 1.6033 - val_weight_output_loss: 1.1918 - val_bag_output_loss: 1.0689 - val_footwear_output_loss: 1.0157 - val_pose_output_loss: 0.5967 - val_emotion_output_loss: 1.0668 - val_gender_output_accuracy: 0.8438 - val_image_quality_output_accuracy: 0.5089 - val_age_output_accuracy: 0.3829 - val_weight_output_accuracy: 0.5739 - val_bag_output_accuracy: 0.6364 - val_footwear_output_accuracy: 0.6141 - val_pose_output_accuracy: 0.8105 - val_emotion_output_accuracy: 0.6443\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162278119361872e-07\n",
            "360/360 - 87s - loss: 2.3948 - gender_output_loss: 0.0527 - image_quality_output_loss: 0.3805 - age_output_loss: 0.6361 - weight_output_loss: 0.3356 - bag_output_loss: 0.1886 - footwear_output_loss: 0.2744 - pose_output_loss: 0.0816 - emotion_output_loss: 0.4451 - gender_output_accuracy: 0.9867 - image_quality_output_accuracy: 0.8891 - age_output_accuracy: 0.7861 - weight_output_accuracy: 0.8995 - bag_output_accuracy: 0.9500 - footwear_output_accuracy: 0.9084 - pose_output_accuracy: 0.9824 - emotion_output_accuracy: 0.8595 - val_loss: 8.0678 - val_gender_output_loss: 0.4318 - val_image_quality_output_loss: 1.1403 - val_age_output_loss: 1.5721 - val_weight_output_loss: 1.1662 - val_bag_output_loss: 1.0921 - val_footwear_output_loss: 1.0169 - val_pose_output_loss: 0.5800 - val_emotion_output_loss: 1.0686 - val_gender_output_accuracy: 0.8492 - val_image_quality_output_accuracy: 0.4881 - val_age_output_accuracy: 0.3740 - val_weight_output_accuracy: 0.6081 - val_bag_output_accuracy: 0.6181 - val_footwear_output_accuracy: 0.6017 - val_pose_output_accuracy: 0.8125 - val_emotion_output_accuracy: 0.6399\n",
            "Epoch 47/200\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162278119361872e-07\n",
            "360/360 - 87s - loss: 2.3683 - gender_output_loss: 0.0526 - image_quality_output_loss: 0.3790 - age_output_loss: 0.6343 - weight_output_loss: 0.3325 - bag_output_loss: 0.1855 - footwear_output_loss: 0.2728 - pose_output_loss: 0.0775 - emotion_output_loss: 0.4341 - gender_output_accuracy: 0.9876 - image_quality_output_accuracy: 0.8865 - age_output_accuracy: 0.7891 - weight_output_accuracy: 0.9022 - bag_output_accuracy: 0.9529 - footwear_output_accuracy: 0.9112 - pose_output_accuracy: 0.9859 - emotion_output_accuracy: 0.8635 - val_loss: 8.1367 - val_gender_output_loss: 0.4594 - val_image_quality_output_loss: 1.1496 - val_age_output_loss: 1.5928 - val_weight_output_loss: 1.1516 - val_bag_output_loss: 1.0944 - val_footwear_output_loss: 1.0222 - val_pose_output_loss: 0.5865 - val_emotion_output_loss: 1.0801 - val_gender_output_accuracy: 0.8418 - val_image_quality_output_accuracy: 0.5174 - val_age_output_accuracy: 0.3795 - val_weight_output_accuracy: 0.6037 - val_bag_output_accuracy: 0.6111 - val_footwear_output_accuracy: 0.6111 - val_pose_output_accuracy: 0.8080 - val_emotion_output_accuracy: 0.6667\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 3.162278119361872e-07\n",
            "360/360 - 87s - loss: 2.3683 - gender_output_loss: 0.0511 - image_quality_output_loss: 0.3786 - age_output_loss: 0.6327 - weight_output_loss: 0.3317 - bag_output_loss: 0.1856 - footwear_output_loss: 0.2738 - pose_output_loss: 0.0776 - emotion_output_loss: 0.4372 - gender_output_accuracy: 0.9887 - image_quality_output_accuracy: 0.8909 - age_output_accuracy: 0.7863 - weight_output_accuracy: 0.8990 - bag_output_accuracy: 0.9557 - footwear_output_accuracy: 0.9109 - pose_output_accuracy: 0.9847 - emotion_output_accuracy: 0.8615 - val_loss: 8.1359 - val_gender_output_loss: 0.4324 - val_image_quality_output_loss: 1.1382 - val_age_output_loss: 1.6110 - val_weight_output_loss: 1.1778 - val_bag_output_loss: 1.1003 - val_footwear_output_loss: 1.0060 - val_pose_output_loss: 0.5929 - val_emotion_output_loss: 1.0772 - val_gender_output_accuracy: 0.8497 - val_image_quality_output_accuracy: 0.4931 - val_age_output_accuracy: 0.3790 - val_weight_output_accuracy: 0.5749 - val_bag_output_accuracy: 0.6042 - val_footwear_output_accuracy: 0.6245 - val_pose_output_accuracy: 0.8125 - val_emotion_output_accuracy: 0.6399\n",
            "Epoch 49/200\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001537946446e-07\n",
            "360/360 - 87s - loss: 2.3707 - gender_output_loss: 0.0536 - image_quality_output_loss: 0.3771 - age_output_loss: 0.6324 - weight_output_loss: 0.3309 - bag_output_loss: 0.1865 - footwear_output_loss: 0.2697 - pose_output_loss: 0.0789 - emotion_output_loss: 0.4416 - gender_output_accuracy: 0.9870 - image_quality_output_accuracy: 0.8889 - age_output_accuracy: 0.7911 - weight_output_accuracy: 0.9008 - bag_output_accuracy: 0.9529 - footwear_output_accuracy: 0.9115 - pose_output_accuracy: 0.9833 - emotion_output_accuracy: 0.8549 - val_loss: 8.1867 - val_gender_output_loss: 0.4539 - val_image_quality_output_loss: 1.1638 - val_age_output_loss: 1.6197 - val_weight_output_loss: 1.1624 - val_bag_output_loss: 1.0810 - val_footwear_output_loss: 1.0402 - val_pose_output_loss: 0.5935 - val_emotion_output_loss: 1.0722 - val_gender_output_accuracy: 0.8477 - val_image_quality_output_accuracy: 0.4886 - val_age_output_accuracy: 0.3765 - val_weight_output_accuracy: 0.6052 - val_bag_output_accuracy: 0.6146 - val_footwear_output_accuracy: 0.6161 - val_pose_output_accuracy: 0.8080 - val_emotion_output_accuracy: 0.6562\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001537946446e-07\n",
            "360/360 - 87s - loss: 2.3842 - gender_output_loss: 0.0519 - image_quality_output_loss: 0.3809 - age_output_loss: 0.6372 - weight_output_loss: 0.3372 - bag_output_loss: 0.1833 - footwear_output_loss: 0.2740 - pose_output_loss: 0.0797 - emotion_output_loss: 0.4400 - gender_output_accuracy: 0.9884 - image_quality_output_accuracy: 0.8860 - age_output_accuracy: 0.7891 - weight_output_accuracy: 0.9002 - bag_output_accuracy: 0.9553 - footwear_output_accuracy: 0.9079 - pose_output_accuracy: 0.9852 - emotion_output_accuracy: 0.8591 - val_loss: 8.3217 - val_gender_output_loss: 0.4644 - val_image_quality_output_loss: 1.1521 - val_age_output_loss: 1.6261 - val_weight_output_loss: 1.2164 - val_bag_output_loss: 1.1318 - val_footwear_output_loss: 1.0180 - val_pose_output_loss: 0.6422 - val_emotion_output_loss: 1.0707 - val_gender_output_accuracy: 0.8393 - val_image_quality_output_accuracy: 0.5005 - val_age_output_accuracy: 0.3785 - val_weight_output_accuracy: 0.5734 - val_bag_output_accuracy: 0.6076 - val_footwear_output_accuracy: 0.6126 - val_pose_output_accuracy: 0.7897 - val_emotion_output_accuracy: 0.6562\n",
            "Epoch 51/200\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 6.52816\n",
            "Learning rate used for the completed epoch: 1.0000001537946446e-07\n",
            "360/360 - 87s - loss: 2.3752 - gender_output_loss: 0.0529 - image_quality_output_loss: 0.3780 - age_output_loss: 0.6350 - weight_output_loss: 0.3375 - bag_output_loss: 0.1841 - footwear_output_loss: 0.2723 - pose_output_loss: 0.0786 - emotion_output_loss: 0.4368 - gender_output_accuracy: 0.9878 - image_quality_output_accuracy: 0.8911 - age_output_accuracy: 0.7848 - weight_output_accuracy: 0.8978 - bag_output_accuracy: 0.9528 - footwear_output_accuracy: 0.9115 - pose_output_accuracy: 0.9843 - emotion_output_accuracy: 0.8633 - val_loss: 8.1950 - val_gender_output_loss: 0.4439 - val_image_quality_output_loss: 1.1685 - val_age_output_loss: 1.6099 - val_weight_output_loss: 1.1568 - val_bag_output_loss: 1.0880 - val_footwear_output_loss: 1.0656 - val_pose_output_loss: 0.5854 - val_emotion_output_loss: 1.0770 - val_gender_output_accuracy: 0.8492 - val_image_quality_output_accuracy: 0.5213 - val_age_output_accuracy: 0.3651 - val_weight_output_accuracy: 0.6151 - val_bag_output_accuracy: 0.6240 - val_footwear_output_accuracy: 0.6096 - val_pose_output_accuracy: 0.8110 - val_emotion_output_accuracy: 0.6701\n",
            "Epoch 52/200\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}